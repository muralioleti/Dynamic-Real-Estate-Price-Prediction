from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Hyperparameters to tune
param_grid_xgb = {
    'n_estimators': [50, 100],
    'learning_rate': [0.01, 0.1],
    'max_depth': [5, 10]
}

# Initialize variables
best_mae = float('inf')
best_params = {}

# Loop through parameters and fit model
for n_estimators in param_grid_xgb['n_estimators']:
    for learning_rate in param_grid_xgb['learning_rate']:
        for max_depth in param_grid_xgb['max_depth']:
            # Initialize the model with the current set of parameters
            model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=42)
            model.fit(X_train, y_train)

            # Predict and calculate MAE
            y_pred = model.predict(X_test)
            mae = mean_absolute_error(y_test, y_pred)

            # If the current MAE is better, update best parameters
            if mae < best_mae:
                best_mae = mae
                best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}

print("Best parameters for XGBoost:", best_params)
